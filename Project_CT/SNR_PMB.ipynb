{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T14:26:48.128693Z",
     "start_time": "2019-06-09T14:26:44.841129Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import random  \n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.layers import LSTM, TimeDistributed, BatchNormalization, Bidirectional, Dense, Dropout\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers import *\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T13:53:46.656944Z",
     "start_time": "2019-06-10T13:53:46.500301Z"
    }
   },
   "outputs": [],
   "source": [
    "UNITS = 32  # Units 数目\n",
    "EPOCHS = 2  # 训练循环数目\n",
    "TEST_SIZE = 0.2  # 数据集分割比例\n",
    "VAL = 0.2  # val 分割比例\n",
    "BATCH_SIZE = 32  # 一次训练多少组\n",
    "seed = 1340  # 随机数种子\n",
    "PATIENCE = 20  # 允许不降落步数\n",
    "OUTPUT_SIZE = 1  # 输入一个 输出 1个\n",
    "LR = 0.006            # 误差期望\n",
    "filepath1 = 'train_3.xlsx'\n",
    "filepath2 = \"Dose_inter_slinear.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T06:39:03.118009Z",
     "start_time": "2019-06-10T06:39:03.107167Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_pre(filepath1, filepath2):\n",
    "    df = pd.read_excel(filepath1)  # 读取文件\n",
    "    training_set = DataFrame(df)\n",
    "    training_set = training_set.T\n",
    "\n",
    "    ct = pd.read_excel(filepath2, header=None)\n",
    "    ct = np.array(ct)\n",
    "    ct = ct[:, :250]\n",
    "\n",
    "    X_train_0 = training_set.iloc[:, :302].values\n",
    "    y_train_0 = training_set.iloc[:, 302:].values\n",
    "    X_train_0 = X_train_0[:, :250]\n",
    "    y_train_0 = y_train_0[:, :250]  # 修建 0 值\n",
    "\n",
    "    X_train_total = np.zeros((200, 250, 2))\n",
    "    for i in range(200):\n",
    "        X_train_total[i, :, 0] = X_train_0[i, :]\n",
    "    for i in range(200):\n",
    "        X_train_total[i, :, 1] = ct[i % 10, :]\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_train_total, y_train_0, test_size=TEST_SIZE, random_state=seed)\n",
    "\n",
    "    #X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], y_train.shape[1], 1))\n",
    "    #X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0], y_test.shape[1], 1))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, X_train_total, y_train_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T15:04:47.604747Z",
     "start_time": "2019-06-10T15:04:46.918599Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(input_shape=(250, 2),\n",
    "        units=UNITS,\n",
    "        return_sequences=True,\n",
    "        name=\"GRU\"))\n",
    "\n",
    "#model.add(Dropout(0.10, name=\"dropout\"))\n",
    "model.add(LSTM(units=UNITS, return_sequences=True, name=\"GRU_2\"))\n",
    "model.add(LSTM(units=UNITS, return_sequences=True, name=\"GRU_3\"))\n",
    "model.add(\n",
    "    TimeDistributed(Dense(OUTPUT_SIZE, name=\"Dense\"), name=\"TDense\"))\n",
    "adam = Adam(LR)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=adam,\n",
    "    loss='mse',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T14:34:25.710441Z",
     "start_time": "2019-06-10T14:34:25.683705Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(\n",
    "    Bidirectional(\n",
    "        GRU(\n",
    "            input_shape=(250, 2),\n",
    "            units=UNITS,\n",
    "            return_sequences=True,\n",
    "            name=\"GRU_1\"),\n",
    "        name=\"BiGRU_1\"))\n",
    "\n",
    "#model.add(Dropout(0.10, name=\"dropout_2\"))\n",
    "\n",
    "model.add(\n",
    "    Bidirectional(\n",
    "        GRU(\n",
    "            input_shape=(250, 2),\n",
    "            units=UNITS,\n",
    "            return_sequences=True,\n",
    "            name=\"GRU_2\"),\n",
    "        name=\"BiGRU_2\"))\n",
    "model.add(\n",
    "    Bidirectional(\n",
    "        GRU(\n",
    "            input_shape=(250, 2),\n",
    "            units=UNITS,\n",
    "            return_sequences=True,\n",
    "            name=\"GRU_3\"),\n",
    "        name=\"BiGRU_3\"))\n",
    "model.add(TimeDistributed(Dense(OUTPUT_SIZE, name=\"Dense\"), name=\"TDense\"))\n",
    "adam = Adam(LR)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=adam,\n",
    "    loss='mse',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T15:05:01.297982Z",
     "start_time": "2019-06-10T15:04:50.502067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples, validate on 32 samples\n",
      "Epoch 1/2\n",
      "128/128 [==============================] - 6s 46ms/step - loss: 0.0785 - val_loss: 0.0508\n",
      "Epoch 2/2\n",
      "128/128 [==============================] - 2s 15ms/step - loss: 0.0470 - val_loss: 0.0441\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, X_train_0, y_train_0 = data_pre(filepath1,filepath2)\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=2,\n",
    "    verbose=1,\n",
    "    validation_split=0.2,  # 加入 交叉验证集\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T15:05:07.439834Z",
     "start_time": "2019-06-10T15:05:06.424582Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights('LSTM_DLoss.h5',by_name= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T14:34:42.317328Z",
     "start_time": "2019-06-10T14:34:42.283322Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def loss_cal(X_test, y_test):\n",
    "\n",
    "    # 计算 LOSS 峰值偏移等指标\n",
    "\n",
    "    N = X_test.shape[0]\n",
    "\n",
    "    test_loss_total = []\n",
    "    peak_loss_total = []\n",
    "    peak_shift_total = []\n",
    "    peak_half_loss_list = []\n",
    "    peak_twenty_percent_loss_list = []\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        X_test[i] = np.array(X_test[i], dtype=float)\n",
    "        y_test[i] = np.array(y_test[i], dtype=float)\n",
    "        X_test_0 = X_test[i][np.newaxis, :, :]\n",
    "        y_test_0 = y_test[i][np.newaxis, :, :]\n",
    "\n",
    "        # 计算 测试集总误差\n",
    "\n",
    "        y_test_0_pred = model.predict(X_test_0)\n",
    "        test_loss = np.mean(np.square(y_test_0_pred - y_test_0))\n",
    "        test_loss_total.append(test_loss)\n",
    "\n",
    "        x_pre = model.predict(X_test_0)\n",
    "        x_pre = x_pre.ravel()  # 调整形状\n",
    "        x_pre = np.array(x_pre, dtype=np.float64)  # 调整 数据\n",
    "        x_real = y_test_0.ravel()\n",
    "\n",
    "        # 计算测试集 peak 偏移\n",
    "        peak_shift = abs(np.argmax(x_real) - np.argmax(x_pre))\n",
    "        peak_shift_total.append(peak_shift)\n",
    "\n",
    "        # 转换为列表\n",
    "        x_real_list = x_real.tolist()\n",
    "        x_pre_list = x_pre.tolist()\n",
    "\n",
    "        # 计算 测试集 距离峰值一半的偏差\n",
    "        half_peak = x_real.max() * 0.5\n",
    "        for j in x_real[:np.argmax(x_real)]:\n",
    "            if j > half_peak or j == half_peak:\n",
    "                index_l1 = x_real_list.index(j)\n",
    "                break\n",
    "        for j in x_real[np.argmax(x_real):]:\n",
    "            if j < half_peak or j == half_peak:\n",
    "                index_r1 = x_real_list[np.argmax(x_real):].index(\n",
    "                    j) + np.argmax(x_real)\n",
    "                break\n",
    "\n",
    "        half_loss = [\n",
    "            abs(x_real_list[i] - x_pre_list[i])\n",
    "            for i in range(index_l1,index_r1)\n",
    "        ]\n",
    "        peak_half_loss = np.mean(np.square(half_loss))\n",
    "        peak_half_loss_list.append(peak_half_loss)\n",
    "\n",
    "        # 计算距离峰值 20% 的偏差\n",
    "        twenty_percent_peak = x_real.max() * 0.2\n",
    "        for i in x_real[:np.argmax(x_real)]:\n",
    "            if i > twenty_percent_peak or i == twenty_percent_peak:\n",
    "                index_l2 = x_real_list.index(i)\n",
    "                break\n",
    "\n",
    "        for i in x_real[np.argmax(x_real):]:\n",
    "            if i < twenty_percent_peak or i == twenty_percent_peak:\n",
    "                index_r2 = x_real_list[np.argmax(x_real):].index(\n",
    "                    i) + np.argmax(x_real)\n",
    "                break\n",
    "        twenty_percent_loss = [\n",
    "            abs(x_real_list[i] - x_pre_list[i])\n",
    "            for i in range(index_l2,index_r2)\n",
    "        ]\n",
    "        peak_twenty_percent_loss = np.mean(np.square(twenty_percent_loss))\n",
    "        peak_twenty_percent_loss_list.append(peak_twenty_percent_loss)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    peak_shift_MAE = np.mean(peak_shift_total)\n",
    "    peak_shift_MSE = np.mean(np.square(peak_shift_total))\n",
    "    peak_twenty_percent_loss_ave = np.mean(peak_twenty_percent_loss_list)\n",
    "    peak_half_loss_ave = np.mean(peak_half_loss_list)\n",
    "    test_loss_total_ave = np.mean(test_loss_total)\n",
    "\n",
    "    return peak_shift_MAE, peak_shift_MSE,test_loss_total_ave, peak_half_loss_ave,peak_twenty_percent_loss_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T14:34:46.934787Z",
     "start_time": "2019-06-10T14:34:46.916285Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def curve_shift(X_test, y_test):\n",
    "\n",
    "    per_average_total = []\n",
    "    per50_average_total = []\n",
    "    per20_average_total = []\n",
    "    N = X_test.shape[0]\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        X_test[i] = np.array(X_test[i], dtype=float)\n",
    "        y_test[i] = np.array(y_test[i], dtype=float)\n",
    "        X_test_0 = X_test[i][np.newaxis, :, :]\n",
    "        y_test_0 = y_test[i][np.newaxis, :, :]\n",
    "        #X_test[i] = X_test[i].reshape(X_test[i].shape(1),X_test[i].shape(0),1)\n",
    "        #y_test[i] = y_test[i].reshape(y_test[i].shape(1),y_test[i].shape(0),1)\n",
    "\n",
    "        \n",
    "\n",
    "        x_pre = model.predict(X_test_0, batch_size=1, verbose=0)\n",
    "        x_pre = x_pre.ravel()  # 调整形状\n",
    "        x_pre = np.array(x_pre, dtype=np.float64)  # 调整 数据\n",
    "        x_real = y_test_0.ravel()\n",
    "\n",
    "        # 计算曲线的 总偏移\n",
    "        #curve_shift = x_real - x_pre\n",
    "        per_list = []\n",
    "        n = 0\n",
    "        for i in range(240):\n",
    "            if x_real[i] >  1e-3:\n",
    "                n= n+1\n",
    "                a= abs(x_real[i] - x_pre[i] )/ x_real[i] \n",
    "                per_list.append(a)\n",
    "        per_average = sum(per_list) / n\n",
    "        per_average_total.append(per_average)\n",
    "\n",
    "        # 计算曲线的半高偏移\n",
    "\n",
    "        # 转换为列表\n",
    "        x_real_list = x_real.tolist()\n",
    "        x_pre_list = x_pre.tolist()\n",
    "\n",
    "        # 计算 测试集 距离峰值一半的偏差\n",
    "        half_peak = x_real.max() * 0.5\n",
    "        for j in x_real[:np.argmax(x_real)]:\n",
    "            if j > half_peak or j == half_peak:\n",
    "                index_l1 = x_real_list.index(j)\n",
    "                break\n",
    "        for j in x_real[np.argmax(x_real):]:\n",
    "            if j < half_peak or j == half_peak:\n",
    "                index_r1 = x_real_list[np.argmax(x_real):].index(\n",
    "                    j) + np.argmax(x_real)\n",
    "                break\n",
    "\n",
    "        per50_list = [\n",
    "            abs(x_real_list[i] - x_pre_list[i]) / x_real_list[i]\n",
    "            for i in range(index_l1,index_r1)\n",
    "        ]\n",
    "\n",
    "        per50_average = sum(per50_list) / (index_r1 - index_l1)\n",
    "        per50_average_total.append(per50_average)\n",
    "\n",
    "        # 计算距离峰值 20% 的偏差\n",
    "        twenty_percent_peak = x_real.max() * 0.2\n",
    "        for i in x_real[:np.argmax(x_real)]:\n",
    "            if i > twenty_percent_peak or i == twenty_percent_peak:\n",
    "                index_l2 = x_real_list.index(i)\n",
    "                break\n",
    "\n",
    "        for i in x_real[np.argmax(x_real):]:\n",
    "            if i < twenty_percent_peak or i == twenty_percent_peak:\n",
    "                index_r2 = x_real_list[np.argmax(x_real):].index(\n",
    "                    i) + np.argmax(x_real)\n",
    "                break\n",
    "        per20_list = [\n",
    "            abs(x_real_list[i] - x_pre_list[i]) / x_real_list[i]\n",
    "            for i in range(index_l2,index_r2)\n",
    "        ]\n",
    "\n",
    "        per20_average = sum(per20_list) / (index_r2 - index_l2)\n",
    "        per20_average_total.append(per20_average)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    per100 = sum(per_average_total) / N\n",
    "    per50 = sum(per50_average_total) / N\n",
    "    per20 = sum(per20_average_total) / N\n",
    "\n",
    "    return per100, per50, per20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T13:59:14.868218Z",
     "start_time": "2019-06-10T13:59:09.127285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.55,\n",
       " 0.65,\n",
       " 0.0005031892497434117,\n",
       " 0.002509077264587034,\n",
       " 0.0004634449879963386)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_cal(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T01:01:28.438839Z",
     "start_time": "2019-06-10T01:01:26.840391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23501883399527218, 0.051138979541799724, 0.055561868605934726)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curve_shift(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T14:54:43.293370Z",
     "start_time": "2019-06-09T14:54:43.288228Z"
    }
   },
   "outputs": [],
   "source": [
    "def activity_add_noise(X_train_0,Inputsize, Inputlength, SNR):\n",
    "    x_train_noise = X_train_0*(np.ones((Inputsize, Inputlength)) + np.random.randn(Inputsize, Inputlength)/SNR)\n",
    "    return x_train_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T01:05:01.750028Z",
     "start_time": "2019-06-10T01:05:01.740461Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_SNR = activity_add_noise(\n",
    "    X_test[:,:,0].reshape(X_test.shape[0], X_test.shape[1]), X_test.shape[0],\n",
    "    X_test.shape[1], 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T01:05:02.618254Z",
     "start_time": "2019-06-10T01:05:02.611461Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_total = np.zeros((40,250,2))\n",
    "for i in range(40):\n",
    "    X_train_total[i,:,0] = X_test_SNR[i,:]\n",
    "for i in range(40):\n",
    "    X_train_total[i,:,1] = X_test[:,:,1][i,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T01:05:11.678385Z",
     "start_time": "2019-06-10T01:05:08.741620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 0.55, 0.0007814954149639123, 0.0046508680941777306, 0.001360477639338479)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_cal(X_train_total,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T01:05:17.181902Z",
     "start_time": "2019-06-10T01:05:15.649530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25482290909677435, 0.07312883025344576, 0.07019523652850454)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curve_shift(X_train_total,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T15:06:28.186376Z",
     "start_time": "2019-06-10T15:05:17.326262Z"
    }
   },
   "outputs": [],
   "source": [
    "peak_shift_MAE_list = []\n",
    "test_loss_total_ave_list = []\n",
    "peak_half_loss_ave_list = []\n",
    "peak_twenty_percent_loss_ave_list = []\n",
    "\n",
    "per100_list = []\n",
    "per50_list = []\n",
    "per20_list = []\n",
    "\n",
    "for i in range(9):\n",
    "    X_test_SNR = activity_add_noise(\n",
    "        X_test[:,:,0].reshape(X_test.shape[0], X_test.shape[1]), X_test.shape[0],\n",
    "        X_test.shape[1], i + 1)\n",
    "    X_train_total = np.zeros((40, 250, 2))\n",
    "    for i in range(40):\n",
    "        X_train_total[i, :, 0] = X_test_SNR[i, :]\n",
    "    for i in range(40):\n",
    "        X_train_total[i, :, 1] = X_test[:, :, 1][i, :]\n",
    "    peak_shift_MAE, peak_shift_MSE, test_loss_total_ave, peak_half_loss_ave, peak_twenty_percent_loss_ave = loss_cal(\n",
    "        X_train_total, y_test)\n",
    "    per100, per50, per20 = curve_shift(X_train_total, y_test)\n",
    "\n",
    "    peak_shift_MAE_list.append(peak_shift_MAE)\n",
    "    test_loss_total_ave_list.append(test_loss_total_ave)\n",
    "    peak_half_loss_ave_list.append(peak_half_loss_ave)\n",
    "    peak_twenty_percent_loss_ave_list.append(peak_twenty_percent_loss_ave)\n",
    "    per100_list.append(per100)\n",
    "    per50_list.append(per50)\n",
    "    per20_list.append(per20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T15:06:30.802430Z",
     "start_time": "2019-06-10T15:06:30.793912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3831591776444699,\n",
       " 0.271376443783086,\n",
       " 0.1718220426538223,\n",
       " 0.13304751989921537,\n",
       " 0.10463109070582861,\n",
       " 0.09681636405121485,\n",
       " 0.0866977241830744,\n",
       " 0.07923593628406388,\n",
       " 0.07899709789992411]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per50_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-10T15:06:45.047947Z",
     "start_time": "2019-06-10T15:06:45.036516Z"
    }
   },
   "outputs": [],
   "source": [
    "GRU_loss_save = pd.DataFrame({\n",
    "    'peak_shift_MAE_list':\n",
    "    peak_shift_MAE_list,\n",
    "    'test_loss_total_ave_list':\n",
    "    test_loss_total_ave_list,\n",
    "    'peak_half_loss_ave_list':\n",
    "    peak_half_loss_ave_list,\n",
    "    'peak_twenty_percent_loss_ave_list':\n",
    "    peak_twenty_percent_loss_ave_list,\n",
    "    'per100_list':\n",
    "    per100_list,\n",
    "    'per50_list':\n",
    "    per50_list,\n",
    "    'per20_list':\n",
    "    per20_list\n",
    "})\n",
    "GRU_loss_save.to_csv(\"LSTM_loss.csv\", index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 592,
   "position": {
    "height": "40px",
    "left": "1070px",
    "right": "20px",
    "top": "120px",
    "width": "351px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
